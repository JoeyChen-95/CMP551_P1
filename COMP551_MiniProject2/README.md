In this project, we first implemented the regular gradient descent (GD) based logistic regression to find an appropriate learning rate and number of iterations. Then, we developed three variants of GD, with minibatch, momentum, and the two combined, to compare their performance and analyze the impacts of the values of parameters on accuracy and speed of convergence. For the minibatch stochastic gradient descent (SGD), we found that under a fixed number of learning rate and training epochs, the smaller the batch size, the higher the accuracy but the lower the convergence speed. For gradient descent with momentum, the value of momentum does not have a large impact on the convergence speed and accuracy in our case. Minibatch SGD with momentum generates the highest accuracy overall. For the second part, we used logistic regression based on term frequencyâ€“inverse document frequency (TF-IDF) to classify whether an article is written by a human or generated by a computer, and we reached an overall accuracy over 80.1\% on the testing dataset. 
